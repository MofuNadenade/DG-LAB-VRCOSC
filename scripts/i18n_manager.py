#!/usr/bin/env python3
"""
æœ¬åœ°åŒ–ç®¡ç†å·¥å…·

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. æå–ä»£ç ä¸­ä½¿ç”¨çš„æœ¬åœ°åŒ–é”®
2. æå–è¯­è¨€æ–‡ä»¶ä¸­å®šä¹‰çš„é”®
3. æŸ¥æ‰¾æœªä½¿ç”¨çš„æœ¬åœ°åŒ–é”®
4. åˆ†ææœ¬åœ°åŒ–é”®çš„ä½¿ç”¨æƒ…å†µ
5. éªŒè¯è¯­è¨€æ–‡ä»¶çš„ä¸€è‡´æ€§
"""

import re
import os
import sys
import argparse
from pathlib import Path
from typing import Set, Dict, List, Tuple, Any
from ruamel.yaml import YAML

yaml_loader = YAML(typ='safe')
yaml_writer = YAML()
yaml_writer.preserve_quotes = True
yaml_writer.default_flow_style = False

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

def flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """å°†åµŒå¥—å­—å…¸æ‰å¹³åŒ–ä¸ºç‚¹åˆ†éš”çš„é”®"""
    items: List[Tuple[str, Any]] = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())  # type: ignore
        else:
            items.append((new_key, v))
    return dict(items)

def extract_keys_from_yaml(file_path: str) -> Set[str]:
    """ä»YAMLæ–‡ä»¶ä¸­æå–æ‰€æœ‰é”®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = yaml_loader.load(f)  # type: ignore
            if data:
                flat_data = flatten_dict(data)  # type: ignore
                return set(flat_data.keys())
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
    return set()

def extract_keys_from_file(file_path: str) -> Set[str]:
    """ä»å•ä¸ªPythonæ–‡ä»¶ä¸­æå–æœ¬åœ°åŒ–é”®"""
    keys: Set[str] = set()
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # åŒ¹é… translate("key") å’Œ translate('key') æ¨¡å¼
            pattern = r'translate\(["\']([^"\']+)["\']\)'
            matches = re.findall(pattern, content)
            keys.update(matches)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
    return keys

def extract_used_keys(src_dir: str) -> Set[str]:
    """æå–srcç›®å½•ä¸‹æ‰€æœ‰Pythonæ–‡ä»¶ä¸­ä½¿ç”¨çš„æœ¬åœ°åŒ–é”®"""
    all_keys: Set[str] = set()
    
    for root, _, files in os.walk(src_dir):
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                keys = extract_keys_from_file(file_path)
                all_keys.update(keys)
    
    return all_keys

def extract_defined_keys(locale_files: List[str]) -> Set[str]:
    """æå–æ‰€æœ‰è¯­è¨€æ–‡ä»¶ä¸­å®šä¹‰çš„é”®"""
    all_defined_keys: Set[str] = set()
    
    for file_path in locale_files:
        if Path(file_path).exists():
            keys = extract_keys_from_yaml(file_path)
            all_defined_keys.update(keys)
    
    return all_defined_keys

def check_consistency(locale_files: List[str]) -> Tuple[bool, List[str]]:
    """æ£€æŸ¥æ‰€æœ‰è¯­è¨€æ–‡ä»¶çš„é”®æ˜¯å¦ä¸€è‡´"""
    errors: List[str] = []
    file_keys: Dict[str, Set[str]] = {}
    
    # æå–æ¯ä¸ªæ–‡ä»¶çš„é”®
    for file_path in locale_files:
        if Path(file_path).exists():
            file_keys[file_path] = extract_keys_from_yaml(file_path)
        else:
            errors.append(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    if len(file_keys) < 2:
        return True, errors
    
    # æ¯”è¾ƒé”®çš„ä¸€è‡´æ€§
    files = list(file_keys.keys())
    base_file = files[0]
    base_keys = file_keys[base_file]
    
    for other_file in files[1:]:
        other_keys = file_keys[other_file]
        
        # æ£€æŸ¥ç¼ºå¤±çš„é”®
        missing_in_other = base_keys - other_keys
        missing_in_base = other_keys - base_keys
        
        if missing_in_other:
            errors.append(f"{other_file} ç¼ºå°‘é”®: {sorted(missing_in_other)}")
        
        if missing_in_base:
            errors.append(f"{base_file} ç¼ºå°‘é”®: {sorted(missing_in_base)}")
    
    return len(errors) == 0, errors

def analyze_usage(src_dir: str, locale_files: List[str]) -> Dict[str, Any]:
    """åˆ†ææœ¬åœ°åŒ–é”®çš„ä½¿ç”¨æƒ…å†µ"""
    used_keys = extract_used_keys(src_dir)
    defined_keys = extract_defined_keys(locale_files)
    unused_keys = defined_keys - used_keys
    missing_keys = used_keys - defined_keys
    
    return {
        'used_keys': used_keys,
        'defined_keys': defined_keys,
        'unused_keys': unused_keys,
        'missing_keys': missing_keys,
        'total_used': len(used_keys),
        'total_defined': len(defined_keys),
        'total_unused': len(unused_keys),
        'total_missing': len(missing_keys)
    }

def print_usage_report(analysis: Dict[str, Any]) -> None:
    """æ‰“å°ä½¿ç”¨æƒ…å†µæŠ¥å‘Š"""
    print("=== æœ¬åœ°åŒ–é”®ä½¿ç”¨æƒ…å†µåˆ†æ ===")
    print(f"å®šä¹‰çš„é”®æ€»æ•°: {analysis['total_defined']}")
    print(f"ä½¿ç”¨çš„é”®æ€»æ•°: {analysis['total_used']}")
    print(f"æœªä½¿ç”¨çš„é”®æ€»æ•°: {analysis['total_unused']}")
    print(f"ç¼ºå¤±çš„é”®æ€»æ•°: {analysis['total_missing']}")
    
    if analysis['unused_keys']:
        print(f"\n=== æœªä½¿ç”¨çš„é”® ({analysis['total_unused']} ä¸ª) ===")
        for key in sorted(analysis['unused_keys']):
            print(key)
    else:
        print("\nâœ… æ‰€æœ‰å®šä¹‰çš„é”®éƒ½è¢«ä½¿ç”¨äº†ï¼")
    
    if analysis['missing_keys']:
        print(f"\n=== ç¼ºå¤±çš„é”® ({analysis['total_missing']} ä¸ª) ===")
        for key in sorted(analysis['missing_keys']):
            print(key)
    else:
        print("\nâœ… æ‰€æœ‰ä½¿ç”¨çš„é”®éƒ½å·²å®šä¹‰ï¼")

def remove_keys_from_dict(data: Any, keys_to_remove: Set[str], parent_key: str = '') -> Dict[str, Any]:
    """ä»åµŒå¥—å­—å…¸ä¸­ç§»é™¤æŒ‡å®šçš„é”®"""
    result: Dict[str, Any] = {}

    for k, v in data.items():
        current_key = f"{parent_key}.{k}" if parent_key else k
        
        if current_key in keys_to_remove:
            continue  # è·³è¿‡è¦åˆ é™¤çš„é”®
        
        if isinstance(v, dict):
            # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
            nested_result = remove_keys_from_dict(v, keys_to_remove, current_key)
            if nested_result:  # åªæœ‰å½“åµŒå¥—å­—å…¸ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ 
                result[k] = nested_result
        else:
            result[k] = v
    
    return result

def clean_unused_keys(locale_files: List[str], unused_keys: Set[str], dry_run: bool = True) -> None:
    """æ¸…ç†æœªä½¿ç”¨çš„é”®"""
    print(f"\n=== æ¸…ç†æœªä½¿ç”¨çš„é”® ({'é¢„è§ˆæ¨¡å¼' if dry_run else 'æ‰§è¡Œæ¨¡å¼'}) ===")
    
    if not unused_keys:
        print("âœ… æ²¡æœ‰æœªä½¿ç”¨çš„é”®éœ€è¦æ¸…ç†ï¼")
        return
    
    for locale_file in locale_files:
        print(f"\nå¤„ç†æ–‡ä»¶: {locale_file}")
        
        try:
            # è¯»å–åŸå§‹æ•°æ®
            with open(locale_file, 'r', encoding='utf-8') as f:
                original_data = yaml_writer.load(f)  # type: ignore
            
            if not original_data:
                print(f"  âš ï¸ æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # ç§»é™¤æœªä½¿ç”¨çš„é”®
            cleaned_data = remove_keys_from_dict(original_data, unused_keys)
            
            # è®¡ç®—ç§»é™¤çš„é”®æ•°é‡
            original_keys = set(flatten_dict(original_data).keys())  # type: ignore
            removed_keys = original_keys & unused_keys
            
            if removed_keys:
                print(f"  å°†ç§»é™¤ {len(removed_keys)} ä¸ªé”®:")
                for key in sorted(removed_keys):
                    print(f"    - {key}")
                
                if not dry_run:
                    # å†™å…¥æ¸…ç†åçš„æ•°æ®
                    with open(locale_file, 'w', encoding='utf-8') as f:
                        yaml_writer.dump(cleaned_data, f)  # type: ignore
                    print(f"  âœ… å·²æ›´æ–°æ–‡ä»¶")
                else:
                    print(f"  ğŸ“‹ é¢„è§ˆå®Œæˆï¼ˆç§»é™¤ --dry-run æ‰§è¡Œå®é™…æ¸…ç†ï¼‰")
            else:
                print(f"  âœ… æ­¤æ–‡ä»¶ä¸­æ²¡æœ‰æœªä½¿ç”¨çš„é”®")
                
        except Exception as e:
            print(f"  âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    if dry_run:
        print(f"\nğŸ“‹ é¢„è§ˆå®Œæˆï¼è¦æ‰§è¡Œå®é™…æ¸…ç†ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
    else:
        print(f"\nğŸ‰ æ¸…ç†å®Œæˆï¼å·²ä»æ‰€æœ‰è¯­è¨€æ–‡ä»¶ä¸­ç§»é™¤ {len(unused_keys)} ä¸ªæœªä½¿ç”¨çš„é”®")

def main() -> None:
    parser = argparse.ArgumentParser(description='æœ¬åœ°åŒ–ç®¡ç†å·¥å…·')
    parser.add_argument('--src-dir', default='src', help='æºä»£ç ç›®å½• (é»˜è®¤: src)')
    parser.add_argument('--locales', nargs='+', 
                       default=['src/locales/zh.yml', 'src/locales/en.yml', 'src/locales/ja.yml'],
                       help='è¯­è¨€æ–‡ä»¶è·¯å¾„åˆ—è¡¨')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…ä¿®æ”¹')
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†ææœ¬åœ°åŒ–é”®ä½¿ç”¨æƒ…å†µ')
    analyze_parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    # æ£€æŸ¥ä¸€è‡´æ€§å‘½ä»¤
    subparsers.add_parser('check', help='æ£€æŸ¥è¯­è¨€æ–‡ä»¶ä¸€è‡´æ€§')
    
    # åˆ—å‡ºä½¿ç”¨çš„é”®å‘½ä»¤
    subparsers.add_parser('list-used', help='åˆ—å‡ºä»£ç ä¸­ä½¿ç”¨çš„æ‰€æœ‰é”®')
    
    # åˆ—å‡ºå®šä¹‰çš„é”®å‘½ä»¤
    subparsers.add_parser('list-defined', help='åˆ—å‡ºè¯­è¨€æ–‡ä»¶ä¸­å®šä¹‰çš„æ‰€æœ‰é”®')
    
    # æŸ¥æ‰¾æœªä½¿ç”¨çš„é”®å‘½ä»¤
    subparsers.add_parser('find-unused', help='æŸ¥æ‰¾æœªä½¿ç”¨çš„é”®')
    
    # æ¸…ç†æœªä½¿ç”¨çš„é”®å‘½ä»¤
    subparsers.add_parser('clean', help='æ¸…ç†æœªä½¿ç”¨çš„é”®')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    src_dir = os.path.abspath(args.src_dir)
    locale_files = [os.path.abspath(f) for f in args.locales]
    
    if args.command == 'analyze':
        analysis = analyze_usage(src_dir, locale_files)
        print_usage_report(analysis)
        
        if args.verbose:
            print(f"\n=== è¯¦ç»†ä¿¡æ¯ ===")
            print(f"æºä»£ç ç›®å½•: {src_dir}")
            print(f"è¯­è¨€æ–‡ä»¶: {locale_files}")
    
    elif args.command == 'check':
        is_consistent, errors = check_consistency(locale_files)
        
        if is_consistent:
            print("âœ… æ‰€æœ‰è¯­è¨€æ–‡ä»¶çš„é”®éƒ½æ˜¯ä¸€è‡´çš„ï¼")
        else:
            print("âŒ è¯­è¨€æ–‡ä»¶é”®ä¸ä¸€è‡´:")
            for error in errors:
                print(f"  - {error}")
    
    elif args.command == 'list-used':
        used_keys = extract_used_keys(src_dir)
        print(f"=== ä»£ç ä¸­ä½¿ç”¨çš„æœ¬åœ°åŒ–é”® ({len(used_keys)} ä¸ª) ===")
        for key in sorted(used_keys):
            print(key)
    
    elif args.command == 'list-defined':
        defined_keys = extract_defined_keys(locale_files)
        print(f"=== è¯­è¨€æ–‡ä»¶ä¸­å®šä¹‰çš„é”® ({len(defined_keys)} ä¸ª) ===")
        for key in sorted(defined_keys):
            print(key)
    
    elif args.command == 'find-unused':
        analysis = analyze_usage(src_dir, locale_files)
        
        if analysis['unused_keys']:
            print(f"=== æœªä½¿ç”¨çš„é”® ({analysis['total_unused']} ä¸ª) ===")
            for key in sorted(analysis['unused_keys']):
                print(key)
        else:
            print("âœ… æ‰€æœ‰é”®éƒ½è¢«ä½¿ç”¨äº†ï¼")
    
    elif args.command == 'clean':
        analysis = analyze_usage(src_dir, locale_files)
        
        if analysis['unused_keys']:
            clean_unused_keys(locale_files, analysis['unused_keys'], dry_run=args.dry_run)
        else:
            print("âœ… æ²¡æœ‰æœªä½¿ç”¨çš„é”®éœ€è¦æ¸…ç†ï¼")

if __name__ == "__main__":
    main()
